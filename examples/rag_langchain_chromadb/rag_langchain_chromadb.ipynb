{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG System Using Langchain, ChromaDB, and OpenAI\n",
    "\n",
    "In this notebook, we'll build a simple rag system using the Langchain library and Chroma vector store. We will:\n",
    "- Load text documents.\n",
    "- Split the text into smaller chunks.\n",
    "- Generate embeddings for the chunks using OpenAI.\n",
    "- Store the embeddings in a Chroma vector database.\n",
    "- Query the database for context-aware answers.\n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Libraries\n",
    "\n",
    "We need to install the required libraries before we proceed. These include `langchain`, `chromadb`, `openai`, and more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install python-dotenv langchain langchain-community langchain-openai unstructured chromadb openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Environment\n",
    "\n",
    "We will load the necessary packages, set up the OpenAI API key, and prepare the folder paths where documents will be stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import shutil\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Define paths\n",
    "CHROMA_PATH = \"./data/chroma\" # Path to store the Chroma vector store\n",
    "DATA_PATH = \"./data/docs\" # Path to the directory containing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loading Documents\n",
    "\n",
    "We will load text documents from the `data/docs/` directory. For this, we'll use the `DirectoryLoader` from Langchain, which can handle multiple files in a directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"./*.txt\", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})\n",
    "    try:\n",
    "        print(\"Starting to load documents...\")\n",
    "        documents = loader.load()\n",
    "        print(\"Documents loaded:\", len(documents))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the document loading process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load documents...\n",
      "Documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting Documents into Chunks\n",
    "\n",
    "Long text documents need to be split into smaller chunks for better processing. We'll use `RecursiveCharacterTextSplitter`, which ensures text chunks remain meaningful and maintain some overlap between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    \n",
    "    # Show an example chunk\n",
    "    document = chunks[2]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the documents into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks...\n",
      "Split 1 documents into 25 chunks.\n",
      "The company prides itself on using cutting-edge technologies to ensure that applications are not only functional but also secure and scalable.\n",
      "Mobile App Development\n",
      "Armada Logics provides comprehensive mobile app development services for both iOS and Android platforms. Their approach includes:\n",
      "{'source': 'data\\\\docs\\\\armada_logics.txt', 'start_index': 1891}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Saving the Chunks to Chroma\n",
    "\n",
    "Now that we've split the documents, we need to generate embeddings for the chunks and store them in the Chroma vector store for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    print(\"Saving chunks to Chroma...\")\n",
    "\n",
    "    # Clear out any existing database\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Store the chunks in Chroma using OpenAI embeddings\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's store the processed chunks into Chroma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks to Chroma...\n",
      "Saved 25 chunks to ./data/chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Local\\Temp\\ipykernel_9408\\2122137945.py:12: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create the prompt with or without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Start with a greeting and provide a thoughtful and friendly response to the question. \n",
    "{context_section}\n",
    "\n",
    "If the answer is unclear or not available, kindly indicate that you don't know. \n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(context, question):\n",
    "    context_section = f\"<context>\\n{context}\\n</context>\" if context else \"\"\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt_text = prompt_template.format(context_section=context_section, question=question)\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Querying the Database\n",
    "\n",
    "Once the chunks are stored, we can perform queries on the database. The following function retrieves the top 3 most relevant chunks based on a query and uses OpenAI to answer based on the context retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def query_database(query_text):\n",
    "\t# Set up the database connection\n",
    "\tembedding_function = OpenAIEmbeddings()\n",
    "\tdb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\t\n",
    "\t# Perform similarity search\n",
    "\tresults = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "\tif results and results[0][1] >= 0.7:\n",
    "\t\tcontext_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\telse:\n",
    "\t\tcontext_text = None\n",
    "\n",
    "\t# Create the prompt\n",
    "\tprompt = create_prompt(context_text, query_text)\n",
    "\tprint(prompt)\n",
    "\t\n",
    "\t# Query the model\n",
    "\tmodel = ChatOpenAI()\n",
    "\tresponse_text = model.invoke(prompt)\n",
    "\t\n",
    "\t# Display response and sources\n",
    "\tsources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\tprint(f\"Response: {response_text}\")\n",
    "\tprint(f\"Sources: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it by asking a question that it probably knows the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Start with a greeting and provide a thoughtful and friendly response to the question. \n",
      "\n",
      "\n",
      "\n",
      "If the answer is unclear or not available, kindly indicate that you don't know. \n",
      "<question>\n",
      "What is the meaning of the universe?\n",
      "</question>\n",
      "\n",
      "Response: content=\"Hello! The meaning of the universe is a complex and philosophical question that has puzzled humans for centuries. It can be interpreted in various ways depending on one's beliefs, values, and perspective. Some see the universe as a result of random chance, while others believe it has a higher purpose or design. Ultimately, the meaning of the universe may be something we may never fully understand.\" response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 58, 'total_tokens': 134}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b03ce106-5463-494c-8974-2aa78f9f002b-0' usage_metadata={'input_tokens': 58, 'output_tokens': 76, 'total_tokens': 134}\n",
      "Sources: ['data\\\\docs\\\\armada_logics.txt', 'data\\\\docs\\\\armada_logics.txt', 'data\\\\docs\\\\armada_logics.txt']\n"
     ]
    }
   ],
   "source": [
    "query_text = \"What is the meaning of the universe?\"\n",
    "query_database(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask a question that it probably doesn't know the answer to, but with our knowledge base set up, it should be able to provide an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Start with a greeting and provide a thoughtful and friendly response to the question. \n",
      "\n",
      "<context>\n",
      "applications for U.S.-based companies, Armada Logics is positioned to deliver high-quality software services that meet the rigorous standards of the tech industry.\n",
      "\n",
      "---\n",
      "\n",
      "Armada Logics is a premier offshore software development company based in the Philippines, founded in 2024 by experienced engineers from Village 88. The company aims to provide Silicon Valley-caliber software solutions, specializing in custom application development, mobile app development, and team\n",
      "\n",
      "---\n",
      "\n",
      "Armada Logics was established by engineers who previously worked at Village 88, a company known for incubating startups and providing software development services. Village 88 has a rich history dating back to 2011, where it began training entry-level developers to meet the demands of Silicon\n",
      "</context>\n",
      "\n",
      "If the answer is unclear or not available, kindly indicate that you don't know. \n",
      "<question>\n",
      "Who is Armada Logics?\n",
      "</question>\n",
      "\n",
      "Response: content='Hello! Armada Logics is a premier offshore software development company based in the Philippines, founded in 2024 by experienced engineers from Village 88. They specialize in custom application development, mobile app development, and team augmentation services. Feel free to reach out if you have any more questions about Armada Logics or their services!' response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 205, 'total_tokens': 272}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-91cdfd3f-0f10-4ab1-a89f-6cca603a0dc7-0' usage_metadata={'input_tokens': 205, 'output_tokens': 67, 'total_tokens': 272}\n",
      "Sources: ['data\\\\docs\\\\armada_logics.txt', 'data\\\\docs\\\\armada_logics.txt', 'data\\\\docs\\\\armada_logics.txt']\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Who is Armada Logics?\"\n",
    "query_database(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully:\n",
    "1. Loaded text documents.\n",
    "2. Split them into smaller chunks.\n",
    "3. Generated embeddings using OpenAI.\n",
    "4. Stored the chunks in Chroma for querying.\n",
    "5. Queried the database for context-based answers.\n",
    "\n",
    "This system can be extended to work with large text datasets, making it useful for various applications like chatbots and question-answering systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
