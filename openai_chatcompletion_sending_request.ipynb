{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completions VS Chat Completions\n",
    "### Completions\n",
    "-  Instead of the input being a list of messages, the input is a freeform text string called a prompt.\n",
    "- Completions uses the `https://api.openai.com/v1/completions` endpoint. \n",
    "- Completions is legacy, meaning it will no longer recieve updates but you still can use it. https://platform.openai.com/docs/guides/text-generation/completions-api\n",
    "- Outdated models.\n",
    "\n",
    "\n",
    "### Chat Completions\n",
    "- Chat models use a list of messages as input and generate a response. They are great for both multi-turn conversations and single-turn tasks.\n",
    "- Chat Completions uses `https://api.openai.com/v1/chat/completions` endpoint.\n",
    "- It's the standard, it's basically used in the official documentation all the time. https://platform.openai.com/docs/quickstart/step-3-sending-your-first-api-request\n",
    "- Latest models and future models will be based on chat completions.\n",
    "\n",
    "### TL;DR\n",
    "- OpenAI announced that they are discontinuing the development of the Completions API. It has been replaced by the Chat Completions API, which was launched in March 2023. https://openai.com/index/gpt-4-api-general-availability/\n",
    "- Completions and Chat Completion each have their own models, which are not interchangeable. https://platform.openai.com/docs/models/model-endpoint-compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completions API\n",
    "- Chat models use a list of messages as input and generate a response. They are great for both multi-turn conversations and single-turn tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the connection to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the .env\n",
    "load_dotenv(find_dotenv())\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Chat Completions API call looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Greetings from the village!\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Response: \", response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The primary input is the `messages` parameter, which must be an array of message objects.\n",
    " - Each message object should include a role (either \"system\", \"user\", or \"assistant\") and content.\n",
    " - Conversations can range from a single message to multiple exchanges between users and assistants.\n",
    " - Typically, conversations begin with a system message, followed by alternating user and assistant messages.\n",
    " - The sampling for `temperature` parameter is typically set between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. \n",
    "  - Read more about the request body and response object of the Chat Completion API : https://platform.openai.com/docs/api-reference/chat/create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function\n",
    "Let's create a simple custom helper function for sending requests to the Chat Completions API. It will make it easier to use prompts and look at the generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(\"Response: \", get_completion(\"Greetings from the village!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the roles: System, User, Assistant\n",
    " - The system role is used to define the behavior of the assistant, such as modifying its personality or providing specific instructions. Note that the system message is optional, and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    " - User role contain previous and current requests or comments for the assistant to respond to.\n",
    " - Assistant role not only store previous responses but can also be crafted by the user to illustrate desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System role vs User role\n",
    "\n",
    "According to the official documentation, the system role is optional, meaning we can ignore it and just use the user role to provide instructions. Read more: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
    "\n",
    "So, instead of using system role like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Hello! My name is Mashiro. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"Your name is Mashiro, you are a helpful assistant\"}, # system role\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Response: \", get_completion(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the user role like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Hello! My name is Mashiro. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Your name is Mashiro, you are a helpful assistant\"}, # changed to user role\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Response: \", get_completion(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I'd say we should stick to using the system role to set the desired behavior of the LLM. Primarly to keep the prompt organized. For developers, we use the system prompt to prime the LLM model with context, and we use the user role for user inputs.\n",
    "\n",
    "Also, using the system role is the standard and widely used in the official documentation, including in the prompt engineering best practices. https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results\n",
    "\n",
    "Try experimenting with the roles, read more:\n",
    "- https://community.openai.com/t/what-exactly-does-a-system-msg-do/459409\n",
    "- https://community.openai.com/t/what-is-the-difference-between-putting-the-ai-personality-in-system-content-and-in-user-content/194938\n",
    "- https://community.openai.com/t/what-should-be-included-in-the-system-part-of-the-prompt/515763\n",
    "- https://community.openai.com/t/the-role-of-system-prompts/149342\n",
    "- https://community.openai.com/t/the-system-role-how-it-influences-the-chat-behavior/87353\n",
    "- https://community.openai.com/t/playground-default-system-message/207937 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
