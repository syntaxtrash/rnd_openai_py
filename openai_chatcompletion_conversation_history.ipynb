{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Memory for LLMs\n",
    "First we need to understand Training an LLM VS Chatting with an LLM.\n",
    " - **Training an LLM**: This is like teaching the LLM. It learns from big sets of information, kind of like reading many books. It becomes smarter over time as it learns more. This is what we call explicit memory (pre-trained data). \n",
    " - **Chatting with an LLM**: This is when we talk with the LLM. It uses what it learned during training to answer our questions or talk with us. But it doesn't remember our past chats unless we send the full chat history. This is what we call implicit memory. By default, LLM models are stateless, each time we interact with it, we have to provide all the necessary information again. Currently, all LLM's work this wayâ€”they need the full set of instructions every time.\n",
    " - **Remember**: LLMs don't remember our past chats automatically. We have to give the full instructions/context if we want them to remember specific things.\n",
    " - Watch this video: https://youtu.be/MmSMAYooRas\n",
    "\n",
    "Read more:\n",
    " - https://simonwillison.net/2024/May/29/training-not-chatting/\n",
    " - https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148/7\n",
    " - https://community.openai.com/t/retain-past-responses-in-memory-without-sending-them-again-at-every-api-request/199647/12\n",
    " - https://community.openai.com/t/is-it-possible-to-reuse-previous-chat-history-on-the-openai-side-to-avoid-sending-repetitive-tokens/206137/6\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the connection to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the .env\n",
    "load_dotenv(find_dotenv())\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Chat Completions API call in python looks like the following, let's ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The recent development of transformer models such as GPT-3 has pushed the boundaries of natural language understanding and generation in AI.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your response is always in once sentence and concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the most exciting recent development in artificial intelligence in one sentence?\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually add the response to the array of message objects and add a few more questions and answers. Lastly, let's ask it what the first question we asked was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Your first question was about the most exciting recent development in artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your response is always in once sentence and concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the most exciting recent development in artificial intelligence in one sentence?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"The recent development of transformer models such as GPT-3 has pushed the boundaries \n",
    "     of natural language understanding and generation in AI.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do large language models like GPT-4 generate human-like text?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"Large language models like GPT-4 generate human-like text through unsupervised \n",
    "\tlearning where they predict the next word in a sequence based on the context provided by the input text.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my first question?\"}, # ask what the first question was\n",
    "\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It understands this because whenever we send a request to the Chat Completion endpoint, we include the messages array containing all our past conversations with the LLM.\n",
    "\n",
    "> Including conversation history is important when user instructions refer to prior messages. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. \n",
    "\n",
    "Read more: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
    "\n",
    "\n",
    "Let's create a helper that automatically adds our input and the response of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # This will be used to format the response into JSON format for better readability.\n",
    "\n",
    "def get_completion(messages, prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Add the user prompt to the messages array\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    # When OpenAI replies, we add that to messages array.\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.choices[0].message.content\n",
    "    })\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Nice to meet you, Aaron! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\", get_completion(messages, \"My name is Aaron\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a helpful assistant.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"My name is Aaron\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Nice to meet you, Aaron! How can I assist you today?\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# See the array of messages objects in better format.\n",
    "print(json.dumps(messages, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making another request, we send the whole array back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Your name is Aaron.\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\", get_completion(messages, \"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a helpful assistant.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"My name is Aaron\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Nice to meet you, Aaron! How can I assist you today?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What is my name?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Your name is Aaron.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(messages, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep talking to the LLM, we have to save the list of message records somewhere, preferably in a database. When we start a new chat with the LLM, it won't remember anything unless we give it the conversation history again. Plus, if the conversation history is too big for the model to handle, we'll have to trim it down somehow. These are both things we need to consider."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
